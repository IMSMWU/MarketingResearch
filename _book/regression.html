<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Regression | Marketing Research Design &amp; Analysis 2019</title>
  <meta name="description" content="An Introduction to Statistics Using R" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Regression | Marketing Research Design &amp; Analysis 2019" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Statistics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Regression | Marketing Research Design &amp; Analysis 2019" />
  
  <meta name="twitter:description" content="An Introduction to Statistics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="anova.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MRDA 2019</a></strong></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html"><i class="fa fa-check"></i>Course materials</a><ul>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#getting-help"><i class="fa fa-check"></i><b>1.2</b> Getting help</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#functions"><i class="fa fa-check"></i><b>1.3</b> Functions</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#packages"><i class="fa fa-check"></i><b>1.4</b> Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#a-typical-r-session"><i class="fa fa-check"></i><b>1.5</b> A typical R session</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>2</b> Data handling</a><ul>
<li class="chapter" data-level="2.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>2.1</b> Basic data handling</a><ul>
<li class="chapter" data-level="2.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>2.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>2.1.2</b> Data types</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>2.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>2.2</b> Data import and export</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>2.2.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-handling.html"><a href="data-handling.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>2.2.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>2.2.3</b> Export data</a></li>
<li class="chapter" data-level="2.2.4" data-path="data-handling.html"><a href="data-handling.html#import-data-from-the-web"><i class="fa fa-check"></i><b>2.2.4</b> Import data from the Web</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-handling.html"><a href="data-handling.html#summarizing-data"><i class="fa fa-check"></i><b>2.3</b> Summarizing data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="data-handling.html"><a href="data-handling.html#summary-statistics"><i class="fa fa-check"></i><b>2.3.1</b> Summary statistics</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-handling.html"><a href="data-handling.html#categorical-variables"><i class="fa fa-check"></i><b>2.3.2</b> Categorical variables</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-handling.html"><a href="data-handling.html#continuous-variables"><i class="fa fa-check"></i><b>2.3.3</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-handling.html"><a href="data-handling.html#data-visualization"><i class="fa fa-check"></i><b>2.4</b> Data visualization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-handling.html"><a href="data-handling.html#categorical-variables-1"><i class="fa fa-check"></i><b>2.4.1</b> Categorical variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-handling.html"><a href="data-handling.html#continuous-variables-1"><i class="fa fa-check"></i><b>2.4.2</b> Continuous variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-handling.html"><a href="data-handling.html#saving-plots"><i class="fa fa-check"></i><b>2.4.3</b> Saving plots</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-handling.html"><a href="data-handling.html#additional-options"><i class="fa fa-check"></i><b>2.4.4</b> Additional options</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-handling.html"><a href="data-handling.html#writing-reports-using-r-markdown"><i class="fa fa-check"></i><b>2.5</b> Writing reports using R-Markdown</a><ul>
<li class="chapter" data-level="2.5.1" data-path="data-handling.html"><a href="data-handling.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i><b>2.5.1</b> Creating a new R-Markdown document</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-handling.html"><a href="data-handling.html#text-and-equations"><i class="fa fa-check"></i><b>2.5.2</b> Text and Equations</a></li>
<li class="chapter" data-level="2.5.3" data-path="data-handling.html"><a href="data-handling.html#r-code"><i class="fa fa-check"></i><b>2.5.3</b> R-Code</a></li>
<li class="chapter" data-level="2.5.4" data-path="data-handling.html"><a href="data-handling.html#latex-math"><i class="fa fa-check"></i><b>2.5.4</b> LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html"><i class="fa fa-check"></i><b>3</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>3.1</b> If we knew it all</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>3.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>3.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>3.3</b> Using what we actually know</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>3.3.1</b> Confidence Intervals for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-statistical-inference.html"><a href="introduction-to-statistical-inference.html#summary"><i class="fa fa-check"></i><b>3.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>4</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>4.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="4.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>4.1.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="4.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#choosing-the-right-test"><i class="fa fa-check"></i><b>4.1.3</b> Choosing the right test</a></li>
<li class="chapter" data-level="4.1.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#summary-1"><i class="fa fa-check"></i><b>4.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>4.2</b> Categorical data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>5</b> Correlation</a><ul>
<li class="chapter" data-level="5.1" data-path="correlation.html"><a href="correlation.html#correlation-coefficient"><i class="fa fa-check"></i><b>5.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="correlation.html"><a href="correlation.html#significance-testing"><i class="fa fa-check"></i><b>5.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="t-test.html"><a href="t-test.html"><i class="fa fa-check"></i><b>6</b> T-Test</a><ul>
<li class="chapter" data-level="6.1" data-path="t-test.html"><a href="t-test.html#one-sample-t-test"><i class="fa fa-check"></i><b>6.1</b> One sample t-test</a></li>
<li class="chapter" data-level="6.2" data-path="t-test.html"><a href="t-test.html#comparing-two-means"><i class="fa fa-check"></i><b>6.2</b> Comparing two means</a><ul>
<li class="chapter" data-level="6.2.1" data-path="t-test.html"><a href="t-test.html#independent-means-t-test"><i class="fa fa-check"></i><b>6.2.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="6.2.2" data-path="t-test.html"><a href="t-test.html#dependent-means-t-test"><i class="fa fa-check"></i><b>6.2.2</b> Dependent-means t-test</a></li>
<li class="chapter" data-level="6.2.3" data-path="t-test.html"><a href="t-test.html#further-considerations"><i class="fa fa-check"></i><b>6.2.3</b> Further considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>7</b> ANOVA</a><ul>
<li class="chapter" data-level="7.1" data-path="anova.html"><a href="anova.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="anova.html"><a href="anova.html#decomposing-variance"><i class="fa fa-check"></i><b>7.2</b> Decomposing variance</a><ul>
<li class="chapter" data-level="7.2.1" data-path="anova.html"><a href="anova.html#total-sum-of-squares"><i class="fa fa-check"></i><b>7.2.1</b> Total sum of squares</a></li>
<li class="chapter" data-level="7.2.2" data-path="anova.html"><a href="anova.html#model-sum-of-squares"><i class="fa fa-check"></i><b>7.2.2</b> Model sum of squares</a></li>
<li class="chapter" data-level="7.2.3" data-path="anova.html"><a href="anova.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>7.2.3</b> Residual sum of squares</a></li>
<li class="chapter" data-level="7.2.4" data-path="anova.html"><a href="anova.html#effect-strength"><i class="fa fa-check"></i><b>7.2.4</b> Effect strength</a></li>
<li class="chapter" data-level="7.2.5" data-path="anova.html"><a href="anova.html#test-of-significance"><i class="fa fa-check"></i><b>7.2.5</b> Test of significance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="anova.html"><a href="anova.html#one-way-anova"><i class="fa fa-check"></i><b>7.3</b> One-way ANOVA</a><ul>
<li class="chapter" data-level="7.3.1" data-path="anova.html"><a href="anova.html#basic-anova"><i class="fa fa-check"></i><b>7.3.1</b> Basic ANOVA</a></li>
<li class="chapter" data-level="7.3.2" data-path="anova.html"><a href="anova.html#post-hoc-tests"><i class="fa fa-check"></i><b>7.3.2</b> Post-hoc tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>8</b> Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>8.1</b> Regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="8.1.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>8.1.2</b> Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>8.2</b> Potential problems</a><ul>
<li class="chapter" data-level="8.2.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>8.2.1</b> Outliers</a></li>
<li class="chapter" data-level="8.2.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>8.2.2</b> Influential observations</a></li>
<li class="chapter" data-level="8.2.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>8.2.3</b> Non-linearity</a></li>
<li class="chapter" data-level="8.2.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>8.2.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="8.2.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>8.2.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="8.2.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>8.2.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="8.2.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>8.2.7</b> Collinearity</a></li>
<li class="chapter" data-level="8.2.8" data-path="regression.html"><a href="regression.html#omitted-variables"><i class="fa fa-check"></i><b>8.2.8</b> Omitted Variables</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>8.3</b> Categorical predictors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>8.3.1</b> Two categories</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>8.3.2</b> More than two categories</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Research Design &amp; Analysis 2019</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">8</span> Regression</h1>
<p>This chapter is primarily based on:</p>
<ul>
<li>Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications (<strong>chapters 6, 7, 8</strong>).</li>
<li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R, Springer (<strong>chapter 3</strong>)</li>
</ul>
<p><a href="./Code/11-regression%20(3).R">You can download the corresponding R-Code here</a></p>
<div id="regression-1" class="section level2">
<h2><span class="header-section-number">8.1</span> Regression</h2>
<p>Correlations measure relationships between variables (i.e., how much two variables covary). Using regression analysis we can predict the outcome of a dependent variable (Y) from one or more independent variables (X). E.g., how many products will we sell if we increase the advertising expenditures by 1000 Euros? In regression analysis, we fit a model to our data and use it to predict the values of the dependent variable from one predictor variable (bivariate regression) or several predictor variables (multiple regression). The following table shows a comparison of correlation and regression analysis:</p>
<p><br></p>
<table>
<colgroup>
<col width="20%" />
<col width="40%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Correlation</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Estimated coefficient</td>
<td>Coefficient of correlation (bounded between -1 and +1)</td>
<td>Regression coefficient (not bounded a priori)</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>Linear association between two variables; Association is bidirectional</td>
<td>(Linear) relation between one or more independent variables and dependent variable; Relation is directional</td>
</tr>
<tr class="odd">
<td>Role of theory</td>
<td>Theory neither required nor testable</td>
<td>Theory required and testable</td>
</tr>
</tbody>
</table>
<p><br></p>
<div id="simple-linear-regression" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Simple linear regression</h3>
<p>In simple linear regression, we assess the relationship between one dependent (regressand) and one independent (regressor) variable. The goal is to fit a line through a scatterplot of observations in order to find the line that best describes the data (scatterplot).</p>
<p>Suppose you are a marketing research analyst at a music label and your task is to suggest, on the basis of past data, a marketing plan for the next year that will maximize product sales. The data set that is available to you includes information on the sales of music downloads (thousands of units), advertising expenditures (in Euros), the number of radio plays an artist received per week (airplay), the number of previous releases of an artist (starpower), repertoire origin (country; 0 = local, 1 = international), and genre (1 = rock, 2 = pop, 3 = electronic). Let’s load and inspect the data first:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1">regression &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/music_sales_regression.dat&quot;</span>, </a>
<a class="sourceLine" id="cb363-2" data-line-number="2">    <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)  <span class="co">#read in data</span></a>
<a class="sourceLine" id="cb363-3" data-line-number="3">regression<span class="op">$</span>country &lt;-<span class="st"> </span><span class="kw">factor</span>(regression<span class="op">$</span>country, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>), </a>
<a class="sourceLine" id="cb363-4" data-line-number="4">    <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;local&quot;</span>, <span class="st">&quot;international&quot;</span>))  <span class="co">#convert grouping variable to factor</span></a>
<a class="sourceLine" id="cb363-5" data-line-number="5">regression<span class="op">$</span>genre &lt;-<span class="st"> </span><span class="kw">factor</span>(regression<span class="op">$</span>genre, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), </a>
<a class="sourceLine" id="cb363-6" data-line-number="6">    <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;rock&quot;</span>, <span class="st">&quot;pop&quot;</span>, <span class="st">&quot;electronic&quot;</span>))  <span class="co">#convert grouping variable to factor</span></a>
<a class="sourceLine" id="cb363-7" data-line-number="7"><span class="kw">head</span>(regression)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["fctr"],"align":["left"]},{"label":["country"],"name":[6],"type":["fctr"],"align":["left"]}],"data":[{"1":"330","2":"10.3","3":"43","4":"10","5":"electronic","6":"international"},{"1":"300","2":"174.1","3":"40","4":"7","5":"electronic","6":"international"},{"1":"250","2":"1000.0","3":"5","4":"7","5":"pop","6":"international"},{"1":"120","2":"75.9","3":"34","4":"6","5":"rock","6":"local"},{"1":"290","2":"1351.3","3":"37","4":"9","5":"electronic","6":"local"},{"1":"60","2":"202.7","3":"13","4":"8","5":"rock","6":"local"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" data-line-number="1">psych<span class="op">::</span><span class="kw">describe</span>(regression)  <span class="co">#descriptive statistics using psych</span></a></code></pre></div>
<pre><code>##           vars   n   mean     sd median trimmed    mad  min  max range
## sales        1 200 193.20  80.70    200  192.69  88.96 10.0  360   350
## adspend      2 200 614.41 485.66    532  560.81 489.09  9.1 2272  2263
## airplay      3 200  27.50  12.27     28   27.46  11.86  0.0   63    63
## starpower    4 200   6.77   1.40      7    6.88   1.48  1.0   10     9
## genre*       5 200   2.40   0.79      3    2.50   0.00  1.0    3     2
## country*     6 200   1.17   0.38      1    1.09   0.00  1.0    2     1
##            skew kurtosis    se
## sales      0.04    -0.72  5.71
## adspend    0.84     0.17 34.34
## airplay    0.06    -0.09  0.87
## starpower -1.27     3.56  0.10
## genre*    -0.83    -0.91  0.06
## country*   1.74     1.05  0.03</code></pre>
<p>As stated above, regression analysis may be used to relate a quantitative response (“dependent variable”) to one or more predictor variables (“independent variables”). In a simple linear regression, we have one dependent and one independent variable.</p>
<p>Here are a few important questions that we might seek to address based on the data:</p>
<ul>
<li>Is there a relationship between advertising budget and sales?</li>
<li>How strong is the relationship between advertising budget and sales?</li>
<li>Which other variables contribute to sales?</li>
<li>How accurately can we estimate the effect of each variable on sales?</li>
<li>How accurately can we predict future sales?</li>
<li>Is the relationship linear?</li>
<li>Is there synergy among the advertising activities?</li>
</ul>
<p>We may use linear regression to answer these questions. Let’s start with the first question and investigate the effect of advertising on sales.</p>
<div id="estimating-the-coefficients" class="section level4">
<h4><span class="header-section-number">8.1.1.1</span> Estimating the coefficients</h4>
<p>A simple linear regression model only has one predictor and can be written as:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation} 
Y=\beta_0+\beta_1X+\epsilon
\tag{8.1}
\end{equation}\]</span></p>
<p>In our specific context, let’s consider only the influence of advertising on sales for now:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\epsilon
\tag{8.2}
\end{equation}\]</span></p>
<p>The word “adspend” represents data on advertising expenditures that we have observed and β<sub>1</sub> (the “slope”“) represents the unknown relationship between advertising expenditures and sales. It tells you by how much sales will increase for an additional Euro spent on advertising. β<sub>0</sub> (the”intercept&quot;) is the number of sales we would expect if no money is spent on advertising. Together, β<sub>0</sub> and β<sub>1</sub> represent the model coefficients or parameters. The error term (ε) captures everything that we miss by using our model, including, (1) misspecifications (the true relationship might not be linear), (2) omitted variables (other variables might drive sales), and (3) measurement error (our measurement of the variables might be imperfect).</p>
<p>Once we have used our training data to produce estimates for the model coefficients, we can predict future sales on the basis of a particular value of advertising expenditures by computing:</p>
<p><span class="math display" id="eq:predreg">\[\begin{equation} 
\hat{Sales}=\hat{\beta_0}+\hat{\beta_1}*adspend
\tag{8.3}
\end{equation}\]</span></p>
<p>We use the hat symbol, <sup>^</sup>, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response (sales). In practice, β<sub>0</sub> and β<sub>1</sub> are unknown and must be estimated from the data to make predictions. In the case of our advertising example, the data set consists of the advertising budget and product sales (n = 200). Our goal is to obtain coefficient estimates such that the linear model fits the available data well. In other words, we fit a line through the scatterplot of observations and try to find the line that best describes the data. The following graph shows the scatterplot for our data, where the black line shows the regression line. The grey vertical lines shows the difference between the predicted values (the regression line) and the observed values. This difference is referred to as the residuals (“e”).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-215"></span>
<img src="_main_files/figure-html/unnamed-chunk-215-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 8.1: Ordinary least squares (OLS)
</p>
</div>
<p>Estimation of the regression function is based on the idea of the method of least squares (OLS = ordinary least squares). The first step is to calculate the residuals by subtracting the observed values from the predicted values.</p>
<p style="text-align:center;">
<span class="math inline">\(e_i = Y_i-(\beta_0+\beta_1X_i)\)</span>
</p>
<p>This difference is then minimized by minimizing the sum of the squared residuals:</p>
<p><span class="math display" id="eq:rss">\[\begin{equation} 
\sum_{i=1}^{N} e_i^2= \sum_{i=1}^{N} [Y_i-(\beta_0+\beta_1X_i)]^2\rightarrow min!
\tag{8.4}
\end{equation}\]</span></p>
<p>e<sub>i</sub>: Residuals (i = 1,2,…,N)<br>
Y<sub>i</sub>: Values of the dependent variable (i = 1,2,…,N) <br>
β<sub>0</sub>: Intercept<br>
β<sub>1</sub>: Regression coefficient / slope parameters<br>
X<sub>ni</sub>: Values of the nth independent variables and the ith observation<br>
N: Number of observations<br></p>
<p>This is also referred to as the <b>residual sum of squares (RSS)</b>. Now we need to choose the values for β<sub>0</sub> and β<sub>1</sub> that minimize RSS. So how can we derive these values for the regression coefficient? The equation for β<sub>1</sub> is given by:</p>
<p><span class="math display" id="eq:slope">\[\begin{equation} 
\hat{\beta_1}=\frac{COV_{XY}}{s_x^2}
\tag{8.5}
\end{equation}\]</span></p>
<p>The exact mathematical derivation of this formula is beyond the scope of this script, but the intuition is to calculate the first derivative of the squared residuals with respect to β<sub>1</sub> and set it to zero, thereby finding the β<sub>1</sub> that minimizes the term. Using the above formula, you can easily compute β<sub>1</sub> using the following code:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1">cov_y_x &lt;-<span class="st"> </span><span class="kw">cov</span>(regression<span class="op">$</span>adspend, regression<span class="op">$</span>sales)</a>
<a class="sourceLine" id="cb366-2" data-line-number="2">cov_y_x</a></code></pre></div>
<pre><code>## [1] 22672</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" data-line-number="1">var_x &lt;-<span class="st"> </span><span class="kw">var</span>(regression<span class="op">$</span>adspend)</a>
<a class="sourceLine" id="cb368-2" data-line-number="2">var_x</a></code></pre></div>
<pre><code>## [1] 235861</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb370-1" data-line-number="1">beta_<span class="dv">1</span> &lt;-<span class="st"> </span>cov_y_x<span class="op">/</span>var_x</a>
<a class="sourceLine" id="cb370-2" data-line-number="2">beta_<span class="dv">1</span></a></code></pre></div>
<pre><code>## [1] 0.0961</code></pre>
<p>The interpretation of β<sub>1</sub> is as follows:</p>
<p>For every extra Euros spent on advertising, sales can be expected to increase by 0.096 units. Or, in other words, if we increase our marketing budget by 1,000 Euros, sales can be expected to increase by 96 units.</p>
<p>Using the estimated coefficient for β<sub>1</sub> , it is easy to compute β<sub>0</sub> (the intercept) as follows:</p>
<p><span class="math display" id="eq:intercept">\[\begin{equation} 
\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}
\tag{8.6}
\end{equation}\]</span></p>
<p>The R code for this is:</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1">beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(regression<span class="op">$</span>sales) <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(regression<span class="op">$</span>adspend)</a>
<a class="sourceLine" id="cb372-2" data-line-number="2">beta_<span class="dv">0</span></a></code></pre></div>
<pre><code>## [1] 134</code></pre>
<p>The interpretation of β<sub>0</sub> is as follows:</p>
<p>If we spend no money on advertising, we would expect to sell 134.14 units.</p>
<p>You may also verify this based on a scatterplot of the data. The following plot shows the scatterplot including the regression line, which is estimated using OLS.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1"><span class="kw">ggplot</span>(regression, <span class="dt">mapping =</span> <span class="kw">aes</span>(adspend, sales)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb374-2" data-line-number="2"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, </a>
<a class="sourceLine" id="cb374-3" data-line-number="3">    <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Advertising expenditures (EUR)&quot;</span>, </a>
<a class="sourceLine" id="cb374-4" data-line-number="4">    <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-218"></span>
<img src="_main_files/figure-html/unnamed-chunk-218-1.png" alt="Scatterplot" width="672" />
<p class="caption">
Figure 8.2: Scatterplot
</p>
</div>
<p>You can see that the regression line intersects with the y-axis at 134.14, which corresponds to the expected sales level when advertising expenditure (on the x-axis) is zero (i.e., the intercept β<sub>0</sub>). The slope coefficient (β<sub>1</sub>) tells you by how much sales (on the y-axis) would increase if advertising expenditures (on the x-axis) are increased by one unit.</p>
</div>
<div id="significance-testing-1" class="section level4">
<h4><span class="header-section-number">8.1.1.2</span> Significance testing</h4>
<p>In a next step, we assess if the effect of advertising on sales is statistically significant. This means that we test the null hypothesis H<sub>0</sub>: “There is no relationship between advertising and sales” versus the alternative hypothesis H<sub>1</sub>: “The is some relationship between advertising and sales”. Or, to state this mathematically:</p>
<p><span class="math display">\[H_0:\beta_1=0\]</span>
<span class="math display">\[H_1:\beta_1\ne0\]</span></p>
<p>How can we test if the effect is statistically significant? Recall the generalized equation to derive a test statistic:</p>
<p><span class="math display" id="eq:teststatgeneral">\[\begin{equation} 
test\ statistic = \frac{effect}{error}
\tag{8.7}
\end{equation}\]</span></p>
<p>The effect is given by the β<sub>1</sub> coefficient in this case. To compute the test statistic, we need to come up with a measure of uncertainty around this estimate (the error). This is because we use information from a sample to estimate the least squares line to make inferences regarding the regression line in the entire population. Since we only have access to one sample, the regression line will be slightly different every time we take a different sample from the population. This is sampling variation and it is perfectly normal! It just means that we need to take into account the uncertainty around the estimate, which is achieved by the standard error. Thus, the test statistic for our hypothesis is given by:</p>
<p><span class="math display" id="eq:teststatreg">\[\begin{equation} 
t = \frac{\hat{\beta_1}}{SE(\hat{\beta_1})}
\tag{8.8}
\end{equation}\]</span></p>
<p>After calculating the test statistic, we compare its value to the values that we would expect to find if there was no effect based on the t-distribution. In a regression context, the degrees of freedom are given by <code>N - p - 1</code> where N is the sample size and p is the number of predictors. In our case, we have 200 observations and one predictor. Thus, the degrees of freedom is 200 - 1 - 1 = 198. In the regression output below, R provides the exact probability of observing a t value of this magnitude (or larger) if the null hypothesis was true. This probability is the p-value. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the outcome variable due to chance in the absence of any real association between the predictor and the outcome.</p>
<p>To estimate the regression model in R, you can use the <code>lm()</code> function. Within the function, you first specify the dependent variable (“sales”) and independent variable (“adspend”) separated by a <code>~</code> (tilde). As mentioned previously, this is known as <em>formula notation</em> in R. The <code>data = regression</code> argument specifies that the variables come from the data frame named “regression”. Strictly speaking, you use the <code>lm()</code> function to create an object called “simple_regression,” which holds the regression output. You can then view the results using the <code>summary()</code> function:</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" data-line-number="1">simple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></a>
<a class="sourceLine" id="cb375-2" data-line-number="2"><span class="kw">summary</span>(simple_regression)  <span class="co">#summary of results</span></a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend, data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -152.95  -43.80   -0.39   37.04  211.87 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 134.13994    7.53657   17.80 &lt;0.0000000000000002 ***
## adspend       0.09612    0.00963    9.98 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 66 on 198 degrees of freedom
## Multiple R-squared:  0.335,  Adjusted R-squared:  0.331 
## F-statistic: 99.6 on 1 and 198 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Note that the estimated coefficients for β<sub>0</sub> (134.14) and β<sub>1</sub> (0.096) correspond to the results of our manual computation above. The associated t-values and p-values are given in the output. The t-values are larger than the critical t-values for the 95% confidence level, since the associated p-values are smaller than 0.05. In case of the coefficient for β<sub>1</sub> this means that the probability of an association between the advertising and sales of the observed magnitude (or larger) is smaller than 0.05, if the value of β<sub>1</sub> was, in fact, 0.</p>
<p>The coefficients associated with the respective variables represent <b>point estimates</b>. To get a better feeling for the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>. A 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>1</sub>, the confidence interval can be computed as.</p>
<p><span class="math display" id="eq:regCI">\[\begin{equation} 
CI = \hat{\beta_1}\pm(t_{1-\frac{\alpha}{2}}*SE(\beta_1))
\tag{8.9}
\end{equation}\]</span></p>
<p>It is easy to compute confidence intervals in R using the <code>confint()</code> function. You just have to provide the name of you estimated model as an argument:</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" data-line-number="1"><span class="kw">confint</span>(simple_regression)</a></code></pre></div>
<pre><code>##                2.5 %  97.5 %
## (Intercept) 119.2777 149.002
## adspend       0.0771   0.115</code></pre>
<p>For our model, the 95% confidence interval for β<sub>0</sub> is [119.28,149], and the 95% confidence interval for β<sub>1</sub> is [0.08,0.12]. Thus, we can conclude that when we do not spend any money on advertising, sales will be somewhere between 119 and 149 units on average. In addition, for each increase in advertising expenditures by one Euro, there will be an average increase in sales of between 0.08 and 0.12.</p>
</div>
<div id="assessing-model-fit" class="section level4">
<h4><span class="header-section-number">8.1.1.3</span> Assessing model fit</h4>
<p>Once we have rejected the null hypothesis in favor of the alternative hypothesis, the next step is to investigate to what extent the model represents (“fits”) the data. How can we assess the model fit?</p>
<ul>
<li>First, we calculate the fit of the most basic model (i.e., the mean)</li>
<li>Then, we calculate the fit of the best model (i.e., the regression model)</li>
<li>A good model should fit the data significantly better than the basic model</li>
<li>R<sup>2</sup>: Represents the percentage of the variation in the outcome that can be explained by the model</li>
<li>The F-ratio measures how much the model has improved the prediction of the outcome compared to the level of inaccuracy in the model</li>
</ul>
<p>Similar to ANOVA, the calculation of model fit statistics relies on estimating the different sum of squares values. SS<sub>T</sub> is the difference between the observed data and the mean value of Y (aka. total variation). In the absence of any other information, the mean value of Y represents the best guess on where an observation at a given level of advertising will fall:</p>
<p><span class="math display" id="eq:regSST">\[\begin{equation} 
SS_T= \sum_{i=1}^{N} (Y_i-\overline{Y})^2
\tag{8.10}
\end{equation}\]</span></p>
<p>The following graph shows the total sum of squares:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-221"></span>
<img src="_main_files/figure-html/unnamed-chunk-221-1.png" alt="Total sum of squares" width="672" />
<p class="caption">
Figure 8.3: Total sum of squares
</p>
</div>
<p>Based on our linear model, the best guess about the sales level at a given level of advertising is the predicted value. The model sum of squares (SS<sub>M</sub>) has the mathematical representation:</p>
<p><span class="math display" id="eq:regSSM">\[\begin{equation} 
SS_M= \sum_{j=1}^{c}  n_j(\overline{Y}_j-\overline{Y})^2
\tag{8.11}
\end{equation}\]</span></p>
<p>The model sum of squares represents the improvement in prediction resulting from using the regression model rather than the mean of the data. The following graph shows the model sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-222"></span>
<img src="_main_files/figure-html/unnamed-chunk-222-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 8.4: Ordinary least squares (OLS)
</p>
</div>
<p>The residual sum of squares (SS<sub>R</sub>) is the difference between the observed data and the predicted values along the regression line (i.e., the variation not explained by the model)</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
SS_R= \sum_{j=1}^{c} \sum_{i=1}^{n} ({Y}_{ij}-\overline{Y}_{j})^2
\tag{8.12}
\end{equation}\]</span></p>
<p>The following graph shows the residual sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-223"></span>
<img src="_main_files/figure-html/unnamed-chunk-223-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 8.5: Ordinary least squares (OLS)
</p>
</div>
<div id="r-squared" class="section level5 unnumbered">
<h5>R-squared</h5>
<p>The R<sup>2</sup> statistic represents the proportion of variance that is explained by the model and is computed as:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
R^2= \frac{SS_M}{SS_T}
\tag{8.12}
\end{equation}\]</span></p>
<p>It takes values between 0 (very bad fit) and 1 (very good fit). Note that when the goal of your model is to predict future outcomes, a “too good” model fit can pose severe challenges. The reason is that the model might fit your specific sample so well, that it will only predict well within the sample but not generalize to other samples. This is called <strong>overfitting</strong> and it shows that there is a trade-off between model fit and out-of-sample predictive ability of the model, if the goal is to predict beyond the sample.</p>
<p>You can get a first impression of the fit of the model by inspecting the scatter plot as can be seen in the plot below. If the observations are highly dispersed around the regression line (left plot), the fit will be lower compared to a data set where the values are less dispersed (right plot).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-224"></span>
<img src="_main_files/figure-html/unnamed-chunk-224-1.png" alt="Good vs. bad model fit" width="960" />
<p class="caption">
Figure 8.6: Good vs. bad model fit
</p>
</div>
<p>The R<sup>2</sup> statistic is reported in the regression output (see above). However, you could also extract the relevant sum of squares statistics from the regression object using the <code>anova()</code> function to compute it manually:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1"><span class="kw">anova</span>(simple_regression)  <span class="co">#anova results</span></a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: sales
##            Df Sum Sq Mean Sq F value              Pr(&gt;F)    
## adspend     1 433688  433688    99.6 &lt;0.0000000000000002 ***
## Residuals 198 862264    4355                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now we can compute R<sup>2</sup> in the same way that we have computed Eta<sup>2</sup> in the last section:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" data-line-number="1">r2 &lt;-<span class="st"> </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>]<span class="op">/</span>(<span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb381-2" data-line-number="2"><span class="st">    </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">2</span>])  <span class="co">#compute R2</span></a></code></pre></div>
</div>
<div id="adjusted-r-squared" class="section level5 unnumbered">
<h5>Adjusted R-squared</h5>
<p>Due to the way the R<sup>2</sup> statistic is calculated, it will never decrease if a new explanatory variable is introduced into the model. This means that every new independent variable either doesn’t change the R<sup>2</sup> or increases it, even if there is no real relationship between the new variable and the dependent variable. Hence, one could be tempted to just add as many variables as possible to increase the R<sup>2</sup> and thus obtain a “better” model. However, this actually only leads to more noise and therefore a worse model.</p>
<p>To account for this, there exists a test statistic closely related to the R<sup>2</sup>, the <strong>adjusted R<sup>2</sup></strong>. It can be calculated as follows:</p>
<p><span class="math display" id="eq:adjustedR2">\[\begin{equation} 
\overline{R^2} = 1 - (1 - R^2)\frac{n-1}{n - k - 1}
\tag{8.13}
\end{equation}\]</span></p>
<p>where <code>n</code> is the total number of observations and <code>k</code> is the total number of explanatory variables. The adjusted R<sup>2</sup> is equal to or less than the regular R<sup>2</sup> and can be negative. It will only increase if the added variable adds more explanatory power than one expect by pure chance. Essentially, it contains a “penalty” for including unnecessary variables and therefore favors more parsimonious models. As such, it is a measure of suitability, good for comparing different models and is very useful in the model selection stage of a project. In R, the standard <code>lm()</code> function automatically also reports the adjusted R<sup>2</sup>.</p>
</div>
<div id="f-test" class="section level5 unnumbered">
<h5>F-test</h5>
<p>Another significance test is the F-test. It tests the null hypothesis:</p>
<p><span class="math display">\[H_0:R^2=0\]</span></p>
<p><br></p>
<p>This is equivalent to the following null hypothesis:</p>
<p><span class="math display">\[H_0:\beta_1=\beta_2=\beta_3=\beta_k=0\]</span></p>
<p><br></p>
<p>The F-test statistic is calculated as follows:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
F=\frac{\frac{SS_M}{k}}{\frac{SS_R}{(n-k-1)}}=\frac{MS_M}{MS_R}
\tag{8.12}
\end{equation}\]</span></p>
<p>which has a F distribution with k number of predictors and n degrees of freedom. In other words, you divide the systematic (“explained”) variation due to the predictor variables by the unsystematic (“unexplained”) variation.</p>
<p>The result of the F-test is provided in the regression output. However, you might manually compute the F-test using the ANOVA results from the model:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" data-line-number="1"><span class="kw">anova</span>(simple_regression)  <span class="co">#anova results</span></a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: sales
##            Df Sum Sq Mean Sq F value              Pr(&gt;F)    
## adspend     1 433688  433688    99.6 &lt;0.0000000000000002 ***
## Residuals 198 862264    4355                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1">f_calc &lt;-<span class="st"> </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">1</span>]<span class="op">/</span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">2</span>]  <span class="co">#compute F</span></a>
<a class="sourceLine" id="cb384-2" data-line-number="2">f_calc</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb386-1" data-line-number="1">f_crit &lt;-<span class="st"> </span><span class="kw">qf</span>(<span class="fl">0.95</span>, <span class="dt">df1 =</span> <span class="dv">1</span>, <span class="dt">df2 =</span> <span class="dv">100</span>)  <span class="co">#critical value</span></a>
<a class="sourceLine" id="cb386-2" data-line-number="2">f_crit</a></code></pre></div>
<pre><code>## [1] 3.9</code></pre>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb388-1" data-line-number="1">f_calc <span class="op">&gt;</span><span class="st"> </span>f_crit  <span class="co">#test if calculated test statistic is larger than critical value</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
</div>
<div id="using-the-model" class="section level4">
<h4><span class="header-section-number">8.1.1.4</span> Using the model</h4>
<p>After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising. Suppose you want to predict sales for a new product, and the company plans to spend 800 Euros on advertising. How much will it sell? You can easily compute this either by hand:</p>
<p><span class="math display">\[\hat{sales}=134.134 + 0.09612*800=211\]</span></p>
<p><br></p>
<p>… or by extracting the estimated coefficients from the model summary:</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb390-1" data-line-number="1"><span class="kw">summary</span>(simple_regression)<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="co"># the intercept</span></a>
<a class="sourceLine" id="cb390-2" data-line-number="2"><span class="kw">summary</span>(simple_regression)<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span><span class="dv">800</span> <span class="co"># the slope * 800</span></a></code></pre></div>
<pre><code>## [1] 211</code></pre>
<p>The predicted value of the dependent variable is 211 units, i.e., the product will (on average) sell 211 units.</p>
<p><strong>The following video summarizes how to conduct simple linear regression in R</strong></p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/HWzvQVqGUzA" frameborder="0" allowfullscreen>
</iframe>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Multiple linear regression</h3>
<p>Multiple linear regression is a statistical technique that simultaneously tests the relationships between two or more independent variables and an interval-scaled dependent variable. The general form of the equation is given by:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation} 
Y=(\beta_0+\beta_1*X_1+\beta_2*X_2+\beta_n*X_n)+\epsilon
\tag{8.1}
\end{equation}\]</span></p>
<p>Again, we aim to find the linear combination of predictors that correlate maximally with the outcome variable. Note that if you change the composition of predictors, the partial regression coefficient of an independent variable will be different from that of the bivariate regression coefficient. This is because the regressors are usually correlated, and any variation in Y that was shared by X1 and X2 was attributed to X1. The interpretation of the partial regression coefficients is the expected change in Y when X is changed by one unit and all other predictors are held constant.</p>
<p>Let’s extend the previous example. Say, in addition to the influence of advertising, you are interested in estimating the influence of airplay on the number of album downloads. The corresponding equation would then be given by:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\beta_2*airplay+\epsilon
\tag{8.2}
\end{equation}\]</span></p>
<p>The words “adspend” and “airplay” represent data that we have observed on advertising expenditures and number of radio plays, and β<sub>1</sub> and β<sub>2</sub> represent the unknown relationship between sales and advertising expenditures and radio airplay, respectively. The coefficients tells you by how much sales will increase for an additional Euro spent on advertising (when radio airplay is held constant) and by how much sales will increase for an additional radio play (when advertising expenditures are held constant). Thus, we can make predictions about album sales based not only on advertising spending, but also on radio airplay.</p>
<p>With several predictors, the partitioning of sum of squares is the same as in the bivariate model, except that the model is no longer a 2-D straight line. With two predictors, the regression line becomes a 3-D regression plane. In our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-229"></span>
<div id="htmlwidget-07bd461d638ca6fde10f" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-07bd461d638ca6fde10f">{"x":{"visdat":{"2f1f415c73a3":["function () ","plotlyVisDat"],"2f1f703ede92":["function () ","data"]},"cur_data":"2f1f703ede92","attrs":{"2f1f415c73a3":{"x":{},"y":{},"z":{},"colors":["#A9D0F5","#08088A"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface"},"2f1f703ede92":{"x":[10.256,174.093,1000,75.896,1351.254,202.705,365.985,305.268,263.268,513.694,152.609,35.987,1720.806,102.568,215.368,426.784,507.772,233.291,1035.433,102.642,526.142,624.538,912.349,611.479,215.994,561.963,474.76,231.523,678.596,70.922,1567.548,263.598,1423.568,715.678,251.192,777.237,509.43,964.11,583.627,923.373,344.392,1095.578,100.025,30.425,1080.342,97.972,799.899,1071.752,893.355,283.161,917.017,234.568,456.897,206.973,1294.099,826.859,406.814,564.158,192.607,10.652,45.689,42.568,20.456,635.192,1002.273,1177.047,507.638,265.398,215.689,526.48,26.895,883.877,9.104,103.568,169.583,429.504,223.639,145.585,1323.287,985.968,500.922,226.652,1051.168,68.093,1547.159,393.774,804.282,801.577,450.562,196.65,26.598,179.061,345.687,295.84,2271.86,1134.575,601.434,45.298,759.518,832.869,1326.598,56.894,709.399,56.895,767.134,503.172,700.929,910.851,888.569,800.615,1500,985.685,1380.689,785.694,792.345,957.167,1789.659,656.137,613.697,313.362,336.51,1544.899,68.954,1445.563,785.692,125.628,377.925,217.994,759.862,1163.444,842.957,125.179,236.598,669.811,1188.193,612.234,922.019,50,2000,1054.027,385.045,1507.972,102.568,204.568,1170.918,574.513,689.547,784.22,405.913,179.778,607.258,1542.329,1112.47,856.985,836.331,236.908,568.954,1077.855,579.321,1500,731.364,25.689,391.749,233.999,275.7,56.895,255.117,471.814,566.501,102.568,250.568,68.594,642.786,1500,102.563,756.984,51.229,644.151,537.352,15.313,243.237,256.894,22.464,45.689,724.938,1126.461,1985.119,1837.516,135.986,514.068,237.703,976.641,1452.689,1600,268.598,900.889,982.063,201.356,746.024,1132.877],"y":[43,40,5,34,37,13,23,54,18,2,11,30,32,22,36,37,9,2,12,5,14,20,57,20,19,35,22,16,53,4,29,43,26,28,24,37,32,34,30,15,23,31,21,28,18,38,28,37,26,30,10,21,18,14,38,36,24,32,9,39,24,45,13,17,32,23,0,25,35,26,19,26,53,29,28,17,26,42,35,17,36,45,20,15,28,27,17,32,46,36,47,19,22,55,31,39,21,36,21,44,27,27,16,33,33,21,35,26,14,34,11,28,33,20,33,28,30,34,49,40,20,42,35,35,8,49,19,42,6,36,32,28,25,34,33,21,34,63,31,25,42,37,25,26,39,44,46,36,12,2,29,33,28,10,38,19,19,13,30,38,22,23,22,20,18,37,16,20,32,26,53,28,32,24,37,30,19,47,22,22,10,1,1,39,8,38,35,40,22,21,27,31,19,24,1,38,26,11,34,55],"z":[330,300,250,120,290,60,140,290,160,100,160,150,290,140,230,230,30,80,190,90,120,150,230,70,150,210,180,140,360,10,240,270,290,220,150,230,220,240,260,170,130,270,140,60,210,190,210,240,210,200,140,90,120,100,360,180,240,150,110,90,160,230,40,60,230,230,120,100,150,120,60,280,120,230,230,40,140,360,250,210,260,250,200,150,250,100,260,210,290,210,220,70,110,250,320,300,180,180,200,320,280,140,100,120,230,150,250,190,240,250,230,120,230,110,210,230,320,210,230,250,60,330,150,360,150,180,80,180,130,320,280,200,130,190,270,150,230,310,340,240,180,220,40,190,290,220,340,250,190,120,230,190,210,170,310,90,170,140,300,340,170,100,200,80,100,70,50,70,240,160,290,140,210,300,230,280,160,200,210,110,110,70,100,190,70,360,360,300,120,200,150,220,280,300,140,290,180,140,210,250],"colors":["#A9D0F5","#08088A"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","marker":{"color":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"size":3,"opacity":0.8,"symbol":75},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"aspectmode":"manual","aspectratio":{"x":1,"y":1,"z":1},"xaxis":{"title":"adspend","range":[0,2300]},"yaxis":{"title":"airplay","range":[0,70]},"zaxis":{"title":"sales","range":0}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"colorbar":{"title":"z","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(169,208,245,1)"],["0.0416666666666667","rgba(164,199,241,1)"],["0.0833333333333334","rgba(159,191,236,1)"],["0.125","rgba(154,182,232,1)"],["0.166666666666667","rgba(149,174,227,1)"],["0.208333333333333","rgba(144,165,223,1)"],["0.25","rgba(139,157,218,1)"],["0.291666666666667","rgba(133,148,214,1)"],["0.333333333333333","rgba(128,140,209,1)"],["0.375","rgba(123,132,205,1)"],["0.416666666666667","rgba(117,124,200,1)"],["0.458333333333333","rgba(112,115,196,1)"],["0.5","rgba(106,107,191,1)"],["0.541666666666667","rgba(101,99,187,1)"],["0.583333333333333","rgba(95,91,183,1)"],["0.625","rgba(89,84,178,1)"],["0.666666666666667","rgba(83,76,174,1)"],["0.708333333333333","rgba(76,68,169,1)"],["0.75","rgba(69,60,165,1)"],["0.791666666666667","rgba(62,52,160,1)"],["0.833333333333333","rgba(55,44,156,1)"],["0.875","rgba(47,36,151,1)"],["0.916666666666667","rgba(37,28,147,1)"],["0.958333333333333","rgba(26,19,142,1)"],["1","rgba(8,8,138,1)"]],"showscale":true,"x":[9.104,103.3855,197.667,291.9485,386.23,480.5115,574.793,669.0745,763.356,857.6375,951.919,1046.2005,1140.482,1234.7635,1329.045,1423.3265,1517.608,1611.8895,1706.171,1800.4525,1894.734,1989.0155,2083.297,2177.5785,2271.86],"y":[0,2.625,5.25,7.875,10.5,13.125,15.75,18.375,21,23.625,26.25,28.875,31.5,34.125,36.75,39.375,42,44.625,47.25,49.875,52.5,55.125,57.75,60.375,63],"z":[[41.9148312294275,50.1066740811475,58.2985169328675,66.4903597845875,74.6822026363075,82.8740454880275,91.0658883397475,99.2577311914674,107.449574043187,115.641416894907,123.833259746627,132.025102598347,140.216945450067,148.408788301787,156.600631153507,164.792474005227,172.984316856947,181.176159708667,189.368002560387,197.559845412107,205.751688263827,213.943531115547,222.135373967267,230.327216818987,238.519059670707],[51.3354036298893,59.5272464816093,67.7190893333293,75.9109321850493,84.1027750367693,92.2946178884893,100.486460740209,108.678303591929,116.870146443649,125.061989295369,133.253832147089,141.445674998809,149.637517850529,157.829360702249,166.021203553969,174.213046405689,182.404889257409,190.596732109129,198.788574960849,206.980417812569,215.172260664289,223.364103516009,231.555946367729,239.747789219449,247.939632071169],[60.7559760303512,68.9478188820712,77.1396617337912,85.3315045855112,93.5233474372311,101.715190288951,109.907033140671,118.098875992391,126.290718844111,134.482561695831,142.674404547551,150.866247399271,159.058090250991,167.249933102711,175.441775954431,183.633618806151,191.825461657871,200.017304509591,208.209147361311,216.400990213031,224.592833064751,232.784675916471,240.976518768191,249.168361619911,257.360204471631],[70.176548430813,78.368391282533,86.560234134253,94.752076985973,102.943919837693,111.135762689413,119.327605541133,127.519448392853,135.711291244573,143.903134096293,152.094976948013,160.286819799733,168.478662651453,176.670505503173,184.862348354893,193.054191206613,201.246034058333,209.437876910053,217.629719761773,225.821562613493,234.013405465213,242.205248316933,250.397091168653,258.588934020373,266.780776872093],[79.5971208312749,87.7889636829948,95.9808065347148,104.172649386435,112.364492238155,120.556335089875,128.748177941595,136.940020793315,145.131863645035,153.323706496755,161.515549348475,169.707392200195,177.899235051915,186.091077903635,194.282920755355,202.474763607075,210.666606458795,218.858449310515,227.050292162235,235.242135013955,243.433977865675,251.625820717395,259.817663569115,268.009506420835,276.201349272555],[89.0176932317367,97.2095360834567,105.401378935177,113.593221786897,121.785064638617,129.976907490337,138.168750342057,146.360593193777,154.552436045497,162.744278897217,170.936121748937,179.127964600657,187.319807452377,195.511650304097,203.703493155817,211.895336007537,220.087178859257,228.279021710977,236.470864562697,244.662707414416,252.854550266136,261.046393117856,269.238235969576,277.430078821296,285.621921673016],[98.4382656321985,106.630108483919,114.821951335639,123.013794187359,131.205637039078,139.397479890798,147.589322742518,155.781165594238,163.973008445958,172.164851297678,180.356694149398,188.548537001118,196.740379852838,204.932222704558,213.124065556278,221.315908407998,229.507751259718,237.699594111438,245.891436963158,254.083279814878,262.275122666598,270.466965518318,278.658808370038,286.850651221758,295.042494073478],[107.85883803266,116.05068088438,124.2425237361,132.43436658782,140.62620943954,148.81805229126,157.00989514298,165.2017379947,173.39358084642,181.58542369814,189.77726654986,197.96910940158,206.1609522533,214.35279510502,222.54463795674,230.73648080846,238.92832366018,247.1201665119,255.31200936362,263.50385221534,271.69569506706,279.88753791878,288.0793807705,296.27122362222,304.46306647394],[117.279410433122,125.471253284842,133.663096136562,141.854938988282,150.046781840002,158.238624691722,166.430467543442,174.622310395162,182.814153246882,191.005996098602,199.197838950322,207.389681802042,215.581524653762,223.773367505482,231.965210357202,240.157053208922,248.348896060642,256.540738912362,264.732581764082,272.924424615802,281.116267467522,289.308110319242,297.499953170962,305.691796022682,313.883638874402],[126.699982833584,134.891825685304,143.083668537024,151.275511388744,159.467354240464,167.659197092184,175.851039943904,184.042882795624,192.234725647344,200.426568499064,208.618411350784,216.810254202504,225.002097054224,233.193939905944,241.385782757664,249.577625609384,257.769468461104,265.961311312824,274.153154164544,282.344997016264,290.536839867984,298.728682719704,306.920525571424,315.112368423144,323.304211274864],[136.120555234046,144.312398085766,152.504240937486,160.696083789206,168.887926640926,177.079769492646,185.271612344366,193.463455196086,201.655298047806,209.847140899526,218.038983751246,226.230826602966,234.422669454686,242.614512306406,250.806355158126,258.998198009846,267.190040861566,275.381883713286,283.573726565006,291.765569416726,299.957412268446,308.149255120166,316.341097971886,324.532940823606,332.724783675326],[145.541127634508,153.732970486228,161.924813337948,170.116656189668,178.308499041388,186.500341893108,194.692184744828,202.884027596548,211.075870448268,219.267713299988,227.459556151708,235.651399003428,243.843241855148,252.035084706868,260.226927558588,268.418770410308,276.610613262028,284.802456113748,292.994298965468,301.186141817187,309.377984668908,317.569827520627,325.761670372347,333.953513224067,342.145356075787],[154.96170003497,163.15354288669,171.34538573841,179.53722859013,187.729071441849,195.92091429357,204.112757145289,212.304599997009,220.496442848729,228.688285700449,236.880128552169,245.071971403889,253.263814255609,261.455657107329,269.647499959049,277.839342810769,286.031185662489,294.223028514209,302.414871365929,310.606714217649,318.798557069369,326.990399921089,335.182242772809,343.374085624529,351.565928476249],[164.382272435431,172.574115287151,180.765958138871,188.957800990591,197.149643842311,205.341486694031,213.533329545751,221.725172397471,229.917015249191,238.108858100911,246.300700952631,254.492543804351,262.684386656071,270.876229507791,279.068072359511,287.259915211231,295.451758062951,303.643600914671,311.835443766391,320.027286618111,328.219129469831,336.410972321551,344.602815173271,352.794658024991,360.986500876711],[173.802844835893,181.994687687613,190.186530539333,198.378373391053,206.570216242773,214.762059094493,222.953901946213,231.145744797933,239.337587649653,247.529430501373,255.721273353093,263.913116204813,272.104959056533,280.296801908253,288.488644759973,296.680487611693,304.872330463413,313.064173315133,321.256016166853,329.447859018573,337.639701870293,345.831544722013,354.023387573733,362.215230425453,370.407073277173],[183.223417236355,191.415260088075,199.607102939795,207.798945791515,215.990788643235,224.182631494955,232.374474346675,240.566317198395,248.758160050115,256.950002901835,265.141845753555,273.333688605275,281.525531456995,289.717374308715,297.909217160435,306.101060012155,314.292902863875,322.484745715595,330.676588567315,338.868431419035,347.060274270755,355.252117122475,363.443959974195,371.635802825915,379.827645677635],[192.643989636817,200.835832488537,209.027675340257,217.219518191977,225.411361043697,233.603203895417,241.795046747137,249.986889598857,258.178732450577,266.370575302297,274.562418154017,282.754261005737,290.946103857457,299.137946709177,307.329789560897,315.521632412617,323.713475264337,331.905318116057,340.097160967777,348.289003819497,356.480846671217,364.672689522937,372.864532374657,381.056375226377,389.248218078097],[202.064562037279,210.256404888999,218.448247740719,226.640090592439,234.831933444159,243.023776295879,251.215619147599,259.407461999319,267.599304851039,275.791147702759,283.982990554479,292.174833406199,300.366676257919,308.558519109639,316.750361961359,324.942204813079,333.134047664799,341.325890516519,349.517733368239,357.709576219959,365.901419071679,374.093261923398,382.285104775118,390.476947626838,398.668790478558],[211.485134437741,219.676977289461,227.868820141181,236.060662992901,244.252505844621,252.444348696341,260.636191548061,268.828034399781,277.0198772515,285.211720103221,293.40356295494,301.59540580666,309.78724865838,317.9790915101,326.17093436182,334.36277721354,342.55462006526,350.74646291698,358.9383057687,367.13014862042,375.32199147214,383.51383432386,391.70567717558,399.8975200273,408.08936287902],[220.905706838202,229.097549689922,237.289392541642,245.481235393362,253.673078245082,261.864921096802,270.056763948522,278.248606800242,286.440449651962,294.632292503682,302.824135355402,311.015978207122,319.207821058842,327.399663910562,335.591506762282,343.783349614002,351.975192465722,360.167035317442,368.358878169162,376.550721020882,384.742563872602,392.934406724322,401.126249576042,409.318092427762,417.509935279482],[230.326279238664,238.518122090384,246.709964942104,254.901807793824,263.093650645544,271.285493497264,279.477336348984,287.669179200704,295.861022052424,304.052864904144,312.244707755864,320.436550607584,328.628393459304,336.820236311024,345.012079162744,353.203922014464,361.395764866184,369.587607717904,377.779450569624,385.971293421344,394.163136273064,402.354979124784,410.546821976504,418.738664828224,426.930507679944],[239.746851639126,247.938694490846,256.130537342566,264.322380194286,272.514223046006,280.706065897726,288.897908749446,297.089751601166,305.281594452886,313.473437304606,321.665280156326,329.857123008046,338.048965859766,346.240808711486,354.432651563206,362.624494414926,370.816337266646,379.008180118366,387.200022970086,395.391865821806,403.583708673526,411.775551525246,419.967394376966,428.159237228686,436.351080080406],[249.167424039588,257.359266891308,265.551109743028,273.742952594748,281.934795446468,290.126638298188,298.318481149908,306.510324001628,314.702166853348,322.894009705068,331.085852556788,339.277695408508,347.469538260228,355.661381111948,363.853223963668,372.045066815388,380.236909667108,388.428752518828,396.620595370548,404.812438222268,413.004281073988,421.196123925708,429.387966777428,437.579809629148,445.771652480868],[258.58799644005,266.77983929177,274.97168214349,283.16352499521,291.35536784693,299.54721069865,307.73905355037,315.93089640209,324.12273925381,332.31458210553,340.50642495725,348.69826780897,356.89011066069,365.08195351241,373.27379636413,381.46563921585,389.65748206757,397.84932491929,406.04116777101,414.23301062273,422.42485347445,430.61669632617,438.80853917789,447.00038202961,455.192224881329],[268.008568840512,276.200411692232,284.392254543952,292.584097395672,300.775940247392,308.967783099112,317.159625950832,325.351468802551,333.543311654272,341.735154505992,349.926997357711,358.118840209431,366.310683061152,374.502525912871,382.694368764591,390.886211616311,399.078054468031,407.269897319751,415.461740171471,423.653583023191,431.845425874911,440.037268726631,448.229111578351,456.420954430071,464.612797281791]],"type":"surface","frame":null},{"x":[10.256,174.093,1000,75.896,1351.254,202.705,365.985,305.268,263.268,513.694,152.609,35.987,1720.806,102.568,215.368,426.784,507.772,233.291,1035.433,102.642,526.142,624.538,912.349,611.479,215.994,561.963,474.76,231.523,678.596,70.922,1567.548,263.598,1423.568,715.678,251.192,777.237,509.43,964.11,583.627,923.373,344.392,1095.578,100.025,30.425,1080.342,97.972,799.899,1071.752,893.355,283.161,917.017,234.568,456.897,206.973,1294.099,826.859,406.814,564.158,192.607,10.652,45.689,42.568,20.456,635.192,1002.273,1177.047,507.638,265.398,215.689,526.48,26.895,883.877,9.104,103.568,169.583,429.504,223.639,145.585,1323.287,985.968,500.922,226.652,1051.168,68.093,1547.159,393.774,804.282,801.577,450.562,196.65,26.598,179.061,345.687,295.84,2271.86,1134.575,601.434,45.298,759.518,832.869,1326.598,56.894,709.399,56.895,767.134,503.172,700.929,910.851,888.569,800.615,1500,985.685,1380.689,785.694,792.345,957.167,1789.659,656.137,613.697,313.362,336.51,1544.899,68.954,1445.563,785.692,125.628,377.925,217.994,759.862,1163.444,842.957,125.179,236.598,669.811,1188.193,612.234,922.019,50,2000,1054.027,385.045,1507.972,102.568,204.568,1170.918,574.513,689.547,784.22,405.913,179.778,607.258,1542.329,1112.47,856.985,836.331,236.908,568.954,1077.855,579.321,1500,731.364,25.689,391.749,233.999,275.7,56.895,255.117,471.814,566.501,102.568,250.568,68.594,642.786,1500,102.563,756.984,51.229,644.151,537.352,15.313,243.237,256.894,22.464,45.689,724.938,1126.461,1985.119,1837.516,135.986,514.068,237.703,976.641,1452.689,1600,268.598,900.889,982.063,201.356,746.024,1132.877],"y":[43,40,5,34,37,13,23,54,18,2,11,30,32,22,36,37,9,2,12,5,14,20,57,20,19,35,22,16,53,4,29,43,26,28,24,37,32,34,30,15,23,31,21,28,18,38,28,37,26,30,10,21,18,14,38,36,24,32,9,39,24,45,13,17,32,23,0,25,35,26,19,26,53,29,28,17,26,42,35,17,36,45,20,15,28,27,17,32,46,36,47,19,22,55,31,39,21,36,21,44,27,27,16,33,33,21,35,26,14,34,11,28,33,20,33,28,30,34,49,40,20,42,35,35,8,49,19,42,6,36,32,28,25,34,33,21,34,63,31,25,42,37,25,26,39,44,46,36,12,2,29,33,28,10,38,19,19,13,30,38,22,23,22,20,18,37,16,20,32,26,53,28,32,24,37,30,19,47,22,22,10,1,1,39,8,38,35,40,22,21,27,31,19,24,1,38,26,11,34,55],"z":[330,300,250,120,290,60,140,290,160,100,160,150,290,140,230,230,30,80,190,90,120,150,230,70,150,210,180,140,360,10,240,270,290,220,150,230,220,240,260,170,130,270,140,60,210,190,210,240,210,200,140,90,120,100,360,180,240,150,110,90,160,230,40,60,230,230,120,100,150,120,60,280,120,230,230,40,140,360,250,210,260,250,200,150,250,100,260,210,290,210,220,70,110,250,320,300,180,180,200,320,280,140,100,120,230,150,250,190,240,250,230,120,230,110,210,230,320,210,230,250,60,330,150,360,150,180,80,180,130,320,280,200,130,190,270,150,230,310,340,240,180,220,40,190,290,220,340,250,190,120,230,190,210,170,310,90,170,140,300,340,170,100,200,80,100,70,50,70,240,160,290,140,210,300,230,280,160,200,210,110,110,70,100,190,70,360,360,300,120,200,150,220,280,300,140,290,180,140,210,250],"type":"scatter3d","mode":"markers","marker":{"color":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"#b31d83",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"size":3,"opacity":0.8,"symbol":75,"line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 8.7: Regression plane
</p>
</div>
<p>Like in the bivariate case, the plane is fitted to the data with the aim to predict the observed data as good as possible. The deviation of the observations from the plane represent the residuals (the error we make in predicting the observed data from the model). Note that this is conceptually the same as in the bivariate case, except that the computation is more complex (we won’t go into details here). The model is fairly easy to plot using a 3-D scatterplot, because we only have two predictors. While multiple regression models that have more than two predictors are not as easy to visualize, you may apply the same principles when interpreting the model outcome:</p>
<ul>
<li>Total sum of squares (SS<sub>T</sub>) is still the difference between the observed data and the mean value of Y (total variation)</li>
<li>Residual sum of squares (SS<sub>R</sub>) is still the difference between the observed data and the values predicted by the model (unexplained variation)</li>
<li>Model sum of squares (SS<sub>M</sub>) is still the difference between the values predicted by the model and the mean value of Y (explained variation)</li>
<li>R measures the multiple correlation between the predictors and the outcome</li>
<li>R<sup>2</sup> is the amount of variation in the outcome variable explained by the model</li>
</ul>
<p>Estimating multiple regression models is straightforward using the <code>lm()</code> function. You just need to separate the individual predictors on the right hand side of the equation using the <code>+</code> symbol. For example, the model:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\beta_2*airplay+\beta_3*starpower+\epsilon
\tag{8.2}
\end{equation}\]</span></p>
<p>could be estimated as follows:</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb392-1" data-line-number="1">multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb392-2" data-line-number="2"><span class="st">    </span>starpower, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></a>
<a class="sourceLine" id="cb392-3" data-line-number="3"><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -121.32  -28.34   -0.45   28.97  144.13 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -26.61296   17.35000   -1.53                 0.13    
## adspend       0.08488    0.00692   12.26 &lt; 0.0000000000000002 ***
## airplay       3.36743    0.27777   12.12 &lt; 0.0000000000000002 ***
## starpower    11.08634    2.43785    4.55            0.0000095 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 47 on 196 degrees of freedom
## Multiple R-squared:  0.665,  Adjusted R-squared:  0.66 
## F-statistic:  129 on 3 and 196 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>The interpretation of the coefficients is as follows:</p>
<ul>
<li>adspend (β<sub>1</sub>): when advertising expenditures increase by 1 Euro, sales will increase by 0.08 units</li>
<li>airplay (β<sub>2</sub>): when radio airplay increases by 1 play per week, sales will increase by 3.37 units</li>
<li>starpower (β<sub>3</sub>): when the number of previous albums increases by 1, sales will increase by 11.09 units</li>
</ul>
<p>The associated t-values and p-values are also given in the output. You can see that the p-values are smaller than 0.05 for all three coefficients. Hence, all effects are “significant”. This means that if the null hypothesis was true (i.e., there was no effect between the variables and sales), the probability of observing associations of the estimated magnitudes (or larger) is very small (e.g., smaller than 0.05).</p>
<p>Again, to get a better feeling for the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>.</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb394-1" data-line-number="1"><span class="kw">confint</span>(multiple_regression)</a></code></pre></div>
<pre><code>##               2.5 % 97.5 %
## (Intercept) -60.830  7.604
## adspend       0.071  0.099
## airplay       2.820  3.915
## starpower     6.279 15.894</code></pre>
<p>What does this tell you? Recall that a 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>3</sub>, the confidence interval is [6.28,15.89]. Thus, although we have computed a point estimate of 11.09 for the effect of starpower on sales based on our sample, the effect might actually just as well take any other value within this range, considering the sample size and the variability in our data.</p>
<p>The output also tells us that 66.47% of the variation can be explained by our model. You may also visually inspect the fit of the model by plotting the predicted values against the observed values. We can extract the predicted values using the <code>predict()</code> function. So let’s create a new variable <code>yhat</code>, which contains those predicted values.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" data-line-number="1">regression<span class="op">$</span>yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(simple_regression)</a></code></pre></div>
<p>We can now use this variable to plot the predicted values against the observed values. In the following plot, the model fit would be perfect if all points would fall on the diagonal line. The larger the distance between the points and the line, the worse the model fit.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb397-1" data-line-number="1"><span class="kw">ggplot</span>(regression,<span class="kw">aes</span>(yhat,sales)) <span class="op">+</span><span class="st">  </span></a>
<a class="sourceLine" id="cb397-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">2</span>,<span class="dt">shape=</span><span class="dv">1</span>) <span class="op">+</span><span class="st">  </span><span class="co">#Use hollow circles</span></a>
<a class="sourceLine" id="cb397-3" data-line-number="3"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name=</span><span class="st">&quot;predicted values&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb397-4" data-line-number="4"><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name=</span><span class="st">&quot;observed values&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb397-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb397-6" data-line-number="6"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-233"></span>
<img src="_main_files/figure-html/unnamed-chunk-233-1.png" alt="Model fit" width="672" />
<p class="caption">
Figure 8.8: Model fit
</p>
</div>
<p><strong>Partial plots</strong></p>
<p>In the context of a simple linear regression (i.e., with a single independent variable), a scatter plot of the dependent variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, however, things become more complicated. The reason is that although the scatter plot still show the relationship between the two variables, it does not take into account the effect of the other independent variables in the model. Partial regression plot show the effect of adding another variable to a model that already controls for the remaining variables in the model. In other words, it is a scatterplot of the residuals of the outcome variable and each predictor when both variables are regressed separately on the remaining predictors. As an example, consider the effect of advertising expenditures on sales. In this case, the partial plot would show the effect of adding advertising expenditures as an explanatory variable while controlling for the variation that is explained by airplay and starpower in both variables (sales and advertising). Think of it as the purified relationship between advertising and sales that remains after controlling for other factors. The partial plots can easily be created using the <code>avPlots()</code> function from the <code>car</code> package:</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb398-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb398-2" data-line-number="2"><span class="kw">avPlots</span>(multiple_regression)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-234"></span>
<img src="_main_files/figure-html/unnamed-chunk-234-1.png" alt="Partial plots" width="672" />
<p class="caption">
Figure 8.9: Partial plots
</p>
</div>
<p><strong>Using the model</strong></p>
<p>After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising, airplay, and starpower. Suppose you would like to predict sales for a new music album with advertising expenditures of 800, airplay of 30 and starpower of 5. How much will it sell?</p>
<p><span class="math display">\[\hat{sales}=−26.61 + 0.084 * 800 + 3.367*30 + 11.08 ∗ 5= 197.74\]</span></p>
<p><br></p>
<p>… or by extracting the estimated coefficients:</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb399-1" data-line-number="1"><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">2</span>, </a>
<a class="sourceLine" id="cb399-2" data-line-number="2">    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">800</span> <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">3</span>, </a>
<a class="sourceLine" id="cb399-3" data-line-number="3">    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">30</span> <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">4</span>, </a>
<a class="sourceLine" id="cb399-4" data-line-number="4">    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">5</span></a></code></pre></div>
<pre><code>## [1] 198</code></pre>
<p>The predicted value of the dependent variable is 198 units, i.e., the product will sell 198 units.</p>
<p><strong>Comparing effects</strong></p>
<p>Using the output from the regression model above, it is difficult to compare the effects of the independent variables because they are all measured on different scales (Euros, radio plays, releases). Standardized regression coefficients can be used to judge the relative importance of the predictor variables. Standardization is achieved by multiplying the unstandardized coefficient by the ratio of the standard deviations of the independent and dependent variables:</p>
<p><span class="math display" id="eq:stdcoeff">\[\begin{equation} 
B_{k}=\beta_{k} * \frac{s_{x_k}}{s_y}
\tag{8.14}
\end{equation}\]</span></p>
<p>Hence, the standardized coefficient will tell you by how many standard deviations the outcome will change as a result of a one standard deviation change in the predictor variable. Standardized coefficients can be easily computed using the <code>lm.beta()</code> function from the <code>lm.beta</code> package.</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb401-1" data-line-number="1"><span class="kw">library</span>(lm.beta)</a>
<a class="sourceLine" id="cb401-2" data-line-number="2"><span class="kw">lm.beta</span>(multiple_regression)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression)
## 
## Standardized Coefficients::
## (Intercept)     adspend     airplay   starpower 
##        0.00        0.51        0.51        0.19</code></pre>
<p>The results show that for <code>adspend</code> and <code>airplay</code>, a change by one standard deviation will result in a 0.51 standard deviation change in sales, whereas for <code>starpower</code>, a one standard deviation change will only lead to a 0.19 standard deviation change in sales. Hence, while the effects of <code>adspend</code> and <code>airplay</code> are comparable in magnitude, the effect of <code>starpower</code> is less strong.</p>
<p><br></p>
<p><strong>The following video summarizes how to conduct multiple regression in R</strong></p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/BMKgcG9me4s" frameborder="0" allowfullscreen>
</iframe>
</div>
</div>
</div>
<div id="potential-problems" class="section level2">
<h2><span class="header-section-number">8.2</span> Potential problems</h2>
<p>Once you have built and estimated your model it is important to run diagnostics to ensure that the results are accurate. In the following section we will discuss common problems.</p>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Outliers</h3>
<p><strong>The following video summarizes how to handle outliers in R</strong></p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/e9lcBwWVdyU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Outliers are data points that differ vastly from the trend. They can introduce bias into a model due to the fact that they alter the parameter estimates. Consider the example below. A linear regression was performed twice on the same data set, except during the second estimation the two green points were changed to be outliers by being moved to the positions indicated in red. The solid red line is the regression line based on the unaltered data set, while the dotted line was estimated using the altered data set. As you can see the second regression would lead to different conclusions than the first. Therefore it is important to identify outliers and further deal with them.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-237"></span>
<img src="_main_files/figure-html/unnamed-chunk-237-1.png" alt="Effects of outliers" width="672" />
<p class="caption">
Figure 8.10: Effects of outliers
</p>
</div>
<p>One quick way to visually detect outliers is by creating a scatterplot (as above) to see whether anything seems off. Another approach is to inspect the studentized residuals. If there are no outliers in your data, about 95% will be between -2 and 2, as per the assumptions of the normal distribution. Values well outside of this range are unlikely to happen by chance and warrant further inspection. As a rule of thumb, observations whose studentized residuals are greater than 3 in absolute values are potential outliers.</p>
<p>The studentized residuals can be obtained in R with the function <code>rstudent()</code>. We can use this function to create a new variable that contains the studentized residuals e music sales regression from before yields the following residuals:</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb403-1" data-line-number="1">regression<span class="op">$</span>stud_resid &lt;-<span class="st"> </span><span class="kw">rstudent</span>(multiple_regression)</a>
<a class="sourceLine" id="cb403-2" data-line-number="2"><span class="kw">head</span>(regression)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["int"],"align":["right"]},{"label":["country"],"name":[6],"type":["int"],"align":["right"]},{"label":["yhat"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["stud_resid"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"330","2":"10","3":"43","4":"10","5":"3","6":"1","7":"135","8":"2.20"},{"1":"300","2":"174","3":"40","4":"7","5":"3","6":"1","7":"229","8":"2.15"},{"1":"250","2":"1000","3":"5","4":"7","5":"2","6":"1","7":"273","8":"2.11"},{"1":"120","2":"76","3":"34","4":"6","5":"1","6":"0","7":"248","8":"-0.87"},{"1":"290","2":"1351","3":"37","4":"9","5":"3","6":"0","7":"189","8":"-0.48"},{"1":"60","2":"203","3":"13","4":"8","5":"1","6":"0","7":"189","8":"-1.36"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>A good way to visually inspect the studentized residuals is to plot them in a scatterplot and roughly check if most of the observations are within the -3, 3 bounds.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" data-line-number="1"><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(regression), regression<span class="op">$</span>stud_resid, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.3</span>, </a>
<a class="sourceLine" id="cb404-2" data-line-number="2">    <span class="fl">3.3</span>))  <span class="co">#create scatterplot </span></a>
<a class="sourceLine" id="cb404-3" data-line-number="3"><span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)  <span class="co">#add reference lines</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-239"></span>
<img src="_main_files/figure-html/unnamed-chunk-239-1.png" alt="Plot of the studentized residuals" width="672" />
<p class="caption">
Figure 8.11: Plot of the studentized residuals
</p>
</div>
<p>To identify potentially influential observations in our data set, we can apply a filter to our data:</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb405-1" data-line-number="1">outliers &lt;-<span class="st"> </span><span class="kw">subset</span>(regression, <span class="kw">abs</span>(stud_resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb405-2" data-line-number="2">outliers</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["int"],"align":["right"]},{"label":["country"],"name":[6],"type":["int"],"align":["right"]},{"label":["yhat"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["stud_resid"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"360","2":"146","3":"42","4":"8","5":"3","6":"0","7":"139","8":"3.2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After a detailed inspection of the potential outliers, you might decide to delete the affected observations from the data set or not. If an outlier has resulted from an error in data collection, then you might simply remove the observation. However, even though data may have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. This means that the decision of whether to exclude an outlier or not is closely related to the question whether this observation is an influential observation, as will be discussed next.</p>
</div>
<div id="influential-observations" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Influential observations</h3>
<p>Related to the issue of outliers is that of influential observations, meaning observations that exert undue influence on the parameters. It is possible to determine whether or not the results are driven by an influential observation by calculating how far the predicted values for your data would move if the model was fitted without this particular observation. This calculated total distance is called <strong>Cook’s distance</strong>. To identify influential observations, we can inspect the respective plots created from the model output. A rule of thumb to determine whether an observation should be classified as influential or not is to look for observation with a Cook’s distance &gt; 1 (although opinions vary on this). The following plot can be used to see the Cook’s distance associated with each data point:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" data-line-number="1"><span class="kw">plot</span>(multiple_regression, <span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-241"></span>
<img src="_main_files/figure-html/unnamed-chunk-241-1.png" alt="Cook's distance" width="672" />
<p class="caption">
Figure 8.12: Cook’s distance
</p>
</div>
<p>It is easy to see that none of the Cook’s distance values is close to the critical value of 1. Another useful plot to identify influential observations is plot number 5 from the output:</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb407-1" data-line-number="1"><span class="kw">plot</span>(multiple_regression, <span class="dv">5</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-242"></span>
<img src="_main_files/figure-html/unnamed-chunk-242-1.png" alt="Residuals vs. Leverage" width="672" />
<p class="caption">
Figure 8.13: Residuals vs. Leverage
</p>
</div>
<p>In this plot, we look for cases outside of a dashed line, which represents <strong>Cook’s distance</strong>. Lines for Cook’s distance thresholds of 0.5 and 1 are included by default. In our example, this line is not even visible, since the Cook’s distance values are far away from the critical values. Generally, you would watch out for outlying values at the upper right corner or at the lower right corner of the plot. Those spots are the places where cases can be influential against a regression line. In our example, there are no influential cases.</p>
<p>To see how influential observations can impact your regression, have a look at <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" target="_blank">this example</a>.</p>
</div>
<div id="non-linearity" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Non-linearity</h3>
<p>An important underlying assumption for OLS is that of linearity, meaning that the relationship between the dependent and the independent variable can be reasonably approximated in linear terms. One quick way to assess whether a linear relationship can be assumed is to inspect the added variable plots that we already came across earlier:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb408-2" data-line-number="2"><span class="kw">avPlots</span>(multiple_regression)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-243"></span>
<img src="_main_files/figure-html/unnamed-chunk-243-1.png" alt="Partial plots" width="672" />
<p class="caption">
Figure 8.14: Partial plots
</p>
</div>
<p>In our example, it appears that linear relationships can be reasonably assumed. Please note, however, that the assumption of linearity implies two things:</p>
<ul>
<li>Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€)</li>
<li>Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return)</li>
</ul>
<p>These assumptions may not be justifiable in certain contexts and you might have to transform your data (e.g., using log-transformations) in these cases, as we will see below.</p>
</div>
<div id="non-constant-error-variance" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Non-constant error variance</h3>
<p><strong>The following video summarizes how to identify non-constant error variance in R</strong></p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/bcYdQlQJZ_U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Another important assumption of the linear model is that the error terms have a constant variance (i.e., homoscedasticity). The following plot from the model output shows the residuals (the vertical distance from an observed value to the predicted values) versus the fitted values (the predicted value from the regression model). If all the points fell exactly on the dashed grey line, it would mean that we have a perfect prediction. The residual variance (i.e., the spread of the values on the y-axis) should be similar across the scale of the fitted values on the x-axis.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb409-1" data-line-number="1"><span class="kw">plot</span>(multiple_regression, <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-244"></span>
<img src="_main_files/figure-html/unnamed-chunk-244-1.png" alt="Residuals vs. fitted values" width="672" />
<p class="caption">
Figure 8.15: Residuals vs. fitted values
</p>
</div>
<p>In our case, this appears to be the case. You can identify non-constant variances in the errors (i.e., heteroscedasticity) from the presence of a funnel shape in the above plot. When the assumption of constant error variances is not met, this might be due to a misspecification of your model (e.g., the relationship might not be linear). In these cases, it often helps to transform your data (e.g., using log-transformations). The red line also helps you to identify potential misspecification of your model. It is a smoothed curve that passes through the residuals and if it lies close to the gray dashed line (as in our case) it suggest a correct specification. If the line would deviate from the dashed grey line a lot (e.g., a U-shape or inverse U-shape), it would suggest that the linear model specification is not reasonable and you should try different specifications.</p>
<p>If OLS is performed despite heteroscedasticity, the estimates of the coefficient will still be correct on average. However, the estimator is <em>inefficient</em>, meaning that the standard error is wrong, which will impact the significance tests (i.e., the p-values will be wrong). However, there are also robust regression methods, which you can use to estimate your model despite the presence of heteroscedasticity.</p>
</div>
<div id="non-normally-distributed-errors" class="section level3">
<h3><span class="header-section-number">8.2.5</span> Non-normally distributed errors</h3>
<p>Another assumption of OLS is that the error term is normally distributed. This can be a reasonable assumption for many scenarios, but we still need a way to check if it is actually the case. As we can not directly observe the actual error term, we have to work with the next best thing - the residuals.</p>
<p>A quick way to assess whether a given sample is approximately normally distributed is by using Q-Q plots. These plot the theoretical position of the observations (under the assumption that they are normally distributed) against the actual position. The plot below is created by the model output and shows the residuals in a Q-Q plot. As you can see, most of the points roughly follow the theoretical distribution, as given by the straight line. If most of the points are close to the line, the data is approximately normally distributed.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" data-line-number="1"><span class="kw">plot</span>(multiple_regression, <span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-245"></span>
<img src="_main_files/figure-html/unnamed-chunk-245-1.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 8.16: Q-Q plot
</p>
</div>
<p>Another way to check for normal distribution of the data is to employ statistical tests that test the null hypothesis that the data is normally distributed, such as the Shapiro–Wilk test. We can extract the residuals from our model using the <code>resid()</code> function and apply the <code>shapiro.test()</code> function to it:</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb411-1" data-line-number="1"><span class="kw">shapiro.test</span>(<span class="kw">resid</span>(multiple_regression))</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(multiple_regression)
## W = 1, p-value = 0.7</code></pre>
<p>As you can see, we can not reject the H<sub>0</sub> of the normally distributed residuals, which means that we can assume the residuals to be approximately normally distributed.</p>
<p>When the assumption of normally distributed errors is not met, this might again be due to a misspecification of your model, in which case it might help to transform your data (e.g., using log-transformations).</p>
</div>
<div id="correlation-of-errors" class="section level3">
<h3><span class="header-section-number">8.2.6</span> Correlation of errors</h3>
<p>The assumption of independent errors implies that for any two observations the residual terms should be uncorrelated. This is also known as a <em>lack of autocorrelation</em>. In theory, this could be tested with the Durbin-Watson test, which checks whether adjacent residuals are correlated. However, be aware that the test is sensitive to the order of your data. Hence, it only makes sense if there is a natural order in the data (e.g., time-series data) when the presence of dependent errors indicates autocorrelation. <strong>Since there is no natural order in our data, we don’t need to apply this test.</strong>.</p>
<p>If you are confronted with data that has a natural order, you can performed the test using the command <code>durbinWatsonTest()</code>, which takes the object that the <code>lm()</code> function generates as an argument. The test statistic varies between 0 and 4, with values close to 2 being desirable. As a rule of thumb values below 1 and above 3 are causes for concern.</p>
</div>
<div id="collinearity" class="section level3">
<h3><span class="header-section-number">8.2.7</span> Collinearity</h3>
<p>Linear dependence of regressors, also known as multicollinearity, is when there is a strong linear relationship between the independent variables. Some correlation will always be present, but severe correlation can make proper estimation impossible. When present, it affects the model in several ways:</p>
<ul>
<li>Limits the size of R<sup>2</sup>: when two variables are highly correlated, the amount of unique explained variance is low; therefore the incremental change in R<sup>2</sup> by including an additional predictor is larger if the predictor is uncorrelated with the other predictors.</li>
<li>Increases the standard errors of the coefficients, making them less trustworthy.</li>
<li>Uncertainty about the importance of predictors: if two predictors explain similar variance in the outcome, we cannot know which of these variables is important.</li>
</ul>
<p>A quick way to find obvious multicollinearity is to examine the correlation matrix of the data. Any value &gt; 0.8 - 0.9 should be cause for concern. You can, for example, create a correlation matrix using the <code>rcorr()</code> function from the <code>Hmisc</code> package.</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb413-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;Hmisc&quot;</span>)</a>
<a class="sourceLine" id="cb413-2" data-line-number="2"><span class="kw">rcorr</span>(<span class="kw">as.matrix</span>(regression[, <span class="kw">c</span>(<span class="st">&quot;adspend&quot;</span>, <span class="st">&quot;airplay&quot;</span>, </a>
<a class="sourceLine" id="cb413-3" data-line-number="3">    <span class="st">&quot;starpower&quot;</span>)]))</a></code></pre></div>
<pre><code>##           adspend airplay starpower
## adspend      1.00    0.10      0.08
## airplay      0.10    1.00      0.18
## starpower    0.08    0.18      1.00
## 
## n= 200 
## 
## 
## P
##           adspend airplay starpower
## adspend           0.1511  0.2557   
## airplay   0.1511          0.0099   
## starpower 0.2557  0.0099</code></pre>
<p>The bivariate correlations can also be show in a plot:</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" data-line-number="1"><span class="kw">plot</span>(regression[, <span class="kw">c</span>(<span class="st">&quot;adspend&quot;</span>, <span class="st">&quot;airplay&quot;</span>, <span class="st">&quot;starpower&quot;</span>)])</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-248"></span>
<img src="_main_files/figure-html/unnamed-chunk-248-1.png" alt="Bivariate correlation plots" width="672" />
<p class="caption">
Figure 8.17: Bivariate correlation plots
</p>
</div>
<p>However, this only spots bivariate multicollinearity. Variance inflation factors can be used to spot more subtle multicollinearity arising from multivariate relationships. It is calculated by regressing X<sub>i</sub> on all other X and using the resulting R<sup>2</sup> to calculate</p>
<p><span class="math display" id="eq:VIF">\[\begin{equation} 
\begin{split}
\frac{1}{1 - R_i^2}
\end{split}
\tag{8.15}
\end{equation}\]</span></p>
<p>VIF values of over 4 are certainly cause for concern and values over 2 should be further investigated. If the average VIF is over 1 the regression may be biased. The VIF for all variables can easily be calculated in R with the <code>vif()</code> function.</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb416-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb416-2" data-line-number="2"><span class="kw">vif</span>(multiple_regression)</a></code></pre></div>
<pre><code>##   adspend   airplay starpower 
##         1         1         1</code></pre>
<p>As you can see the values are well below the cutoff, indicating that we do not have to worry about multicollinearity in our example.</p>
</div>
<div id="omitted-variables" class="section level3">
<h3><span class="header-section-number">8.2.8</span> Omitted Variables</h3>
<p>If a variable that influences the outcome is left out of the model (“omitted”), a bias in other variables’ coefficients might be introduced. Specifically, the other coefficients will be biased if the corresponding variables are correlated with the omitted variable. Intuitively, the variables left in the model “pick up” the effect of the omitted variable to the degree that they are related. Let’s illustrate this with an example.</p>
<p>Consider the following data on the number of people visiting concerts of smaller bands.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb418-1" data-line-number="1"><span class="kw">head</span>(concerts)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["avg_rating"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["followers"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["concert_visitors"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.88","2":"6903","3":"1425"},{"1":"7.88","2":"5760","3":"1317"},{"1":"4.09","2":"6164","3":"1311"},{"1":"8.83","2":"12198","3":"2612"},{"1":"9.40","2":"7702","3":"1712"},{"1":"0.46","2":"9051","3":"1818"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The data set contains three variables:</p>
<ul>
<li><strong>avg_rating</strong>: The average rating a band has, resulting from a ten-point scale.</li>
<li><strong>followers</strong>: The number of followers the band has at the time of the concert.</li>
<li><strong>concert_visitors</strong>: The number of tickets sold for the concert.</li>
</ul>
<p>If we estimate a model to explain the number of tickets sold as a function of the average rating and the number of followers, the results would look as follows:</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1">concert_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(concert_visitors <span class="op">~</span><span class="st"> </span>avg_rating <span class="op">+</span><span class="st"> </span>followers, </a>
<a class="sourceLine" id="cb419-2" data-line-number="2">    <span class="dt">data =</span> concerts)</a>
<a class="sourceLine" id="cb419-3" data-line-number="3"><span class="kw">summary</span>(concert_mod)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = concert_visitors ~ avg_rating + followers, data = concerts)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -37.99 -13.64  -2.17  11.50  67.33 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) -2.810495   5.778273   -0.49                0.63    
## avg_rating  20.512461   0.708420   28.96 &lt;0.0000000000000002 ***
## followers    0.199940   0.000755  265.00 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20 on 97 degrees of freedom
## Multiple R-squared:  0.999,  Adjusted R-squared:  0.999 
## F-statistic: 3.95e+04 on 2 and 97 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Now assume we don’t have data on the number of followers a band has, but we still have information on the average rating and want to explain the number of tickets sold. Fitting a linear model with just the <code>avg_rating</code> variable included yields the following results:</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" data-line-number="1">concert_mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(concert_visitors <span class="op">~</span><span class="st"> </span>avg_rating, <span class="dt">data =</span> concerts)</a>
<a class="sourceLine" id="cb421-2" data-line-number="2"><span class="kw">summary</span>(concert_mod2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = concert_visitors ~ avg_rating, data = concerts)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1040.1  -403.6   -16.7   360.7   952.9 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)   1114.8      105.8    10.5 &lt; 0.0000000000000002 ***
## avg_rating      64.5       18.4     3.5              0.00071 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 523 on 98 degrees of freedom
## Multiple R-squared:  0.111,  Adjusted R-squared:  0.102 
## F-statistic: 12.2 on 1 and 98 DF,  p-value: 0.00071</code></pre>
<p>What happens to the coefficient of <code>avg_rating</code>? Because <code>avg_rating</code> and <code>followers</code>are not independent (e.g. one could argue that bands with a higher average rating probably have more followers) the coefficient will be biased. In our case we massively overestimate the effect that the average rating of a band has on ticket sales. In the original model, the effect was about 20.5. In the new, smaller model, the effect is approximately 3.1 times higher.</p>
<p>We can also work out intuitively what the bias will be. The marginal effect of <code>followers</code> on <code>concert_visitors</code> is captured by <code>avg_rating</code> to the degree that <code>avg_rating</code> is related to <code>followers</code>. There are two coefficients of interest:</p>
<ol style="list-style-type: decimal">
<li>What is the marginal effect of <code>followers</code> on <code>concert_visitors</code>?</li>
<li>How much of that effect is captured by <code>avg_rating</code>?</li>
</ol>
<p>The former is just the coefficient of <code>followers</code> in the original regression.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" data-line-number="1">followers_concert_mod &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(concert_mod)[<span class="st">&quot;followers&quot;</span>])  <span class="co"># Marginal effect of followers on sales</span></a>
<a class="sourceLine" id="cb423-2" data-line-number="2">followers_concert_mod</a></code></pre></div>
<pre><code>## [1] 0.2</code></pre>
<p>The latter is the coefficient of <code>avg_rating</code> obtained from a regression on <code>followers</code>, since the coefficient shows how <code>avg_rating</code> and <code>followers</code> relate to each other.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb425-1" data-line-number="1">mod_follow_rating &lt;-<span class="st"> </span><span class="kw">lm</span>(followers <span class="op">~</span><span class="st"> </span>avg_rating, <span class="dt">data =</span> concerts)</a>
<a class="sourceLine" id="cb425-2" data-line-number="2">follow_rating &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(mod_follow_rating)[<span class="st">&quot;avg_rating&quot;</span>])  <span class="co"># Relation between followers and avg_rating</span></a>
<a class="sourceLine" id="cb425-3" data-line-number="3">follow_rating</a></code></pre></div>
<pre><code>## [1] 220</code></pre>
<p>Now we can calculate the bias induced by omitting <code>followers</code></p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" data-line-number="1">bias &lt;-<span class="st"> </span>followers_concert_mod <span class="op">*</span><span class="st"> </span>follow_rating</a>
<a class="sourceLine" id="cb427-2" data-line-number="2"><span class="kw">cat</span>(<span class="st">&quot;Bias:&quot;</span>, bias)</a></code></pre></div>
<pre><code>## Bias: 44</code></pre>
<p>To calculate the biased coefficient, simply add the bias to the coefficient from the original model.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" data-line-number="1">b1_bias &lt;-<span class="st"> </span><span class="kw">coef</span>(concert_mod)[<span class="st">&quot;avg_rating&quot;</span>] <span class="op">+</span><span class="st"> </span>bias</a>
<a class="sourceLine" id="cb429-2" data-line-number="2"><span class="kw">cat</span>(<span class="st">&quot;Biased Beta:&quot;</span>, b1_bias)</a></code></pre></div>
<pre><code>## Biased Beta: 65</code></pre>
</div>
</div>
<div id="categorical-predictors" class="section level2">
<h2><span class="header-section-number">8.3</span> Categorical predictors</h2>
<div id="two-categories" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Two categories</h3>
<p>Suppose, you wish to investigate the effect of the variable “country” on sales, which is a categorical variable that can only take two levels (i.e., 0 = local artist, 1 = international artist). Categorical variables with two levels are also called binary predictors. It is straightforward to include these variables in your model as “dummy” variables. Dummy variables are factor variables that can only take two values. For our “country” variable, we can create a new predictor variable that takes the form:</p>
<p><span class="math display" id="eq:dummycoding">\[\begin{equation} 
x_4 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th artist is international}\\
    0  &amp; \quad \text{if } i \text{th artist is local}
  \end{cases}
\tag{8.16}
\end{equation}\]</span></p>
<p>This new variable is then added to our regression equation from before, so that the equation becomes</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*international+\epsilon
\end{align}\]</span></p>
<p>where “international” represents the new dummy variable and <span class="math inline">\(\beta_4\)</span> is the coefficient associated with this variable. Estimating the model is straightforward - you just need to include the variable as an additional predictor variable. Note that the variable needs to be specified as a factor variable before including it in your model. If you haven’t converted it to a factor variable before, you could also use the wrapper function <code>as.factor()</code> within the equation.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" data-line-number="1">multiple_regression_bin &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb431-2" data-line-number="2"><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></a>
<a class="sourceLine" id="cb431-3" data-line-number="3"><span class="kw">summary</span>(multiple_regression_bin)  <span class="co">#summary of results</span></a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country, 
##     data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -109.20  -24.30   -1.82   29.19  156.31 
## 
## Coefficients:
##                       Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)          -16.40060   16.39540   -1.00                 0.32    
## adspend                0.08146    0.00653   12.48 &lt; 0.0000000000000002 ***
## airplay                3.03766    0.26809   11.33 &lt; 0.0000000000000002 ***
## starpower             10.08100    2.29546    4.39           0.00001843 ***
## countryinternational  45.67274    8.69117    5.26           0.00000039 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 44 on 195 degrees of freedom
## Multiple R-squared:  0.706,  Adjusted R-squared:   0.7 
## F-statistic:  117 on 4 and 195 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>You can see that we now have an additional coefficient in the regression output, which tells us the effect of the binary predictor. The dummy variable can generally be interpreted as the average difference in the dependent variable between the two groups (similar to a t-test). In this case, the coefficient tells you the difference in sales between international and local artists, and whether this difference is significant. Specifically, it means that international artists on average sell 45.67 units more than local artists, and this difference is significant (i.e., p &lt; 0.05).</p>
</div>
<div id="more-than-two-categories" class="section level3">
<h3><span class="header-section-number">8.3.2</span> More than two categories</h3>
<p>Predictors with more than two categories, like our “genre”&quot; variable, can also be included in your model. However, in this case one dummy variable cannot represent all possible values, since there are three genres (i.e., 1 = Rock, 2 = Pop, 3 = Electronic). Thus, we need to create additional dummy variables. For example, for our “genre” variable, we create two dummy variables as follows:</p>
<p><span class="math display" id="eq:dummycoding1">\[\begin{equation} 
x_5 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is from Pop genre}\\
    0  &amp; \quad \text{if } i \text{th product is from Rock genre}
  \end{cases}
\tag{8.17}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:dummycoding2">\[\begin{equation} 
x_6 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is from Electronic genre}\\
    0  &amp; \quad \text{if } i \text{th product is from Rock genre}
  \end{cases}
\tag{8.18}
\end{equation}\]</span></p>
<p>We would then add these variables as additional predictors in the regression equation and obtain the following model</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*international\\
      &amp;+\beta_5*Pop\\
      &amp;+\beta_6*Electronic+\epsilon
\end{align}\]</span></p>
<p>where “Pop” and “Rock” represent our new dummy variables, and <span class="math inline">\(\beta_5\)</span> and <span class="math inline">\(\beta_6\)</span> represent the associated regression coefficients.</p>
<p>The interpretation of the coefficients is as follows: <span class="math inline">\(\beta_5\)</span> is the difference in average sales between the genres “Rock” and “Pop”, while <span class="math inline">\(\beta_6\)</span> is the difference in average sales between the genres “Rock” and “Electro”. Note that the level for which no dummy variable is created is also referred to as the <em>baseline</em>. In our case, “Rock” would be the baseline genre. This means that there will always be one fewer dummy variable than the number of levels.</p>
<p>You don’t have to create the dummy variables manually as R will do this automatically when you add the variable to your equation:</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" data-line-number="1">multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb433-2" data-line-number="2"><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span>genre, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></a>
<a class="sourceLine" id="cb433-3" data-line-number="3"><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country + 
##     genre, data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -116.18  -26.54    0.05   27.98  154.56 
## 
## Coefficients:
##                       Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)          -30.67901   16.59989   -1.85              0.06611 .  
## adspend                0.07233    0.00657   11.00 &lt; 0.0000000000000002 ***
## airplay                2.71418    0.26824   10.12 &lt; 0.0000000000000002 ***
## starpower             10.49628    2.19380    4.78            0.0000034 ***
## countryinternational  40.87988    8.40868    4.86            0.0000024 ***
## genrepop              47.69640   10.48717    4.55            0.0000095 ***
## genreelectronic       27.62034    8.17223    3.38              0.00088 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 42 on 193 degrees of freedom
## Multiple R-squared:  0.735,  Adjusted R-squared:  0.727 
## F-statistic: 89.2 on 6 and 193 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>How can we interpret the coefficients? It is estimated based on our model that products from the “Pop” genre will on average sell 47.69 units more than products from the “Rock” genre, and that products from the “Electronic” genre will sell on average 27.62 units more than the products from the “Rock” genre. The p-value of both variables is smaller than 0.05, suggesting that there is statistical evidence for a real difference in sales between the genres.</p>
<p>The level of the baseline category is arbitrary. As you have seen, R simply selects the first level as the baseline. If you would like to use a different baseline category, you can use the <code>relevel()</code> function and set the reference category using the <code>ref</code> argument. The following would estimate the same model using the second category as the baseline:</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" data-line-number="1">multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb435-2" data-line-number="2"><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span><span class="kw">relevel</span>(genre, <span class="dt">ref =</span> <span class="dv">2</span>), </a>
<a class="sourceLine" id="cb435-3" data-line-number="3">    <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></a>
<a class="sourceLine" id="cb435-4" data-line-number="4"><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country + 
##     relevel(genre, ref = 2), data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -116.18  -26.54    0.05   27.98  154.56 
## 
## Coefficients:
##                                    Estimate Std. Error t value
## (Intercept)                        17.01739   18.19704    0.94
## adspend                             0.07233    0.00657   11.00
## airplay                             2.71418    0.26824   10.12
## starpower                          10.49628    2.19380    4.78
## countryinternational               40.87988    8.40868    4.86
## relevel(genre, ref = 2)rock       -47.69640   10.48717   -4.55
## relevel(genre, ref = 2)electronic -20.07606    7.98747   -2.51
##                                               Pr(&gt;|t|)    
## (Intercept)                                      0.351    
## adspend                           &lt; 0.0000000000000002 ***
## airplay                           &lt; 0.0000000000000002 ***
## starpower                                    0.0000034 ***
## countryinternational                         0.0000024 ***
## relevel(genre, ref = 2)rock                  0.0000095 ***
## relevel(genre, ref = 2)electronic                0.013 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 42 on 193 degrees of freedom
## Multiple R-squared:  0.735,  Adjusted R-squared:  0.727 
## F-statistic: 89.2 on 6 and 193 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Note that while your choice of the baseline category impacts the coefficients and the significance level, the prediction for each group will be the same regardless of this choice.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="anova.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
